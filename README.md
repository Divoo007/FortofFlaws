# PolyHack23

# FlawFort

## Libraries and Packages required:
gTTS (pip install gTTS)
playsound (pip install playsound)
Keras (pip install keras)
Computer Vision (pip install opencv-python)
TensorFlow (pip install tensorflow)
NumPy (pip install numpy)

## Introduction:

Communication is a very important and necessary skill in today's world. Communication can effeciently help us communicate with multiple people around the world, make friends, make us socially extroverted, and most of all, motivate and leave an impact around everyone us!
Our aim for this hackathon is not only theorising, but also building a system that can potentially help disabled people be more socially open, and develop the skills that we thought were difficult for them!

## YouTube:

https://youtu.be/AMmiPiulEqQ - Air-canvas
https://youtu.be/6Y5m62OF8rI - Sign Language Detection

## About this system:

In an era of technological advancement, an innovative system is emerging that aims to bridge communication gaps for individuals with disabilities. By combining computer vision and machine learning, this system seeks to convert sign language into spoken words using a camera. This groundbreaking technology has the potential to revolutionize the lives of those who use sign language as their primary mode of communication. By capturing and analyzing hand gestures and facial expressions, the camera interprets sign language and employs machine learning algorithms to generate clear and intelligible speech. Please also note that to make a system this advanced, it takes years of time to train the model. Since we were only given a weeks time, it was difficult to train a model that fast. This is why we will see that the model is not very powerfull. It may mistake some letters that look almost alike, like A and T, or C and O.

For the second system, we have an air canvas. If a deaf person is not familiar with sign language, then how exactly would he communicate with others? This is where our air canvas comes in! In this air canvas, the disabled needs to wear a ring, and define the color of it. Then our system will automatically trace the path of this ring, and show you what the disabled is drawing and what he is trying to communicated

Confused as to how these systems work? head over to "youtube link here" and find out how each of these systems work! you can even test the code above.

## Vision:
My vision is to create a world where disabled people do not need to be socially awkward anymore, and can develop these special skills that we find very crucial!

## How we achieved it:
### Training:
The first phase of our code involves training a computer vision model that can successfully recognise and learn sign language - this will be trained on quite a bit of images to ensure correct responses!
### Model:
The second phase of our code will be to build a code that will recognise the trained images, outline it and tell us what the word/letter/alphabet is
### NLP:
The third phase will be use a powerfull Natural Language Processing module to speak the generated text as and when the user is speaking sign language
